<head>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>

学习笔记参考视频：
[https://www.bilibili.com/video/BV1LE411G7Xj](https://www.bilibili.com/video/BV1LE411G7Xj)

资料参考：
[http://incompleteideas.net/book/RLbook2018.pdf](http://incompleteideas.net/book/RLbook2018.pdf)


## 一、马尔可夫决策过程背景

###1、决策过程的准备工作
&ensp;&ensp;马尔可夫决策过程需要经历两个相关的准备工作$马尔可夫链(Markov Chain/Process) \rightarrow 马尔可夫奖励过程(Markov Reward Process) \rightarrow 马尔可夫决策过程(Markov Decision Process)$；我们需要了解决策评估的价值函数，以及马尔可夫决策中的控制函数（决策迭代和价值迭代）。

Fig1

&ensp;&ensp;如Fig1所示，我们可以将决策过程通过马尔可夫的决策过程来表示，**马尔可夫的决策过程是强化学习的基本框架**，在马尔可夫决策过程中，环境是可观测的，但是环境中的部分变量无法表示，但是这部分无法观测的问题可以转换到马尔可夫决策中。

&ensp;&ensp;**马尔可夫特征：**
1. 历史特征可以用$h\_t={s\_1,s\_2,s\_3,......,s\_t}$来进行表示。

2. 当且仅当满足如下两个特征的马尔可夫特征$$p(s\_{t+1}|{s\_t})=p(s\_{t+1}|h\_{t}).......(1)$$$$p(s\_{t+1}|{s\_t},a\_t)=p(s\_{t+1}|h\_t,a\_t).......(2)$$

3. 将来的特征是独立于当前的特征的      

###2、马尔可夫链(Markov Process/Markov Chain)

Fig2

&ensp;&ensp;如上图Fig2为状态之间的转移概率$p(s\_{t+1}=s^/|s\_t=s)$，则条件转移概率如Fig3所示

**马尔可夫链举例：**
Fig4

&ensp;&ensp;Fig4中是简单的状态转移的概率，转到相邻状态以及转到自己状态的概率。假设我们从S3出发，随机得到长度为5的路径序列，如下：

- S3,S4,S5,S6,S6

- S3,S2,S3,S2,S1

- S3,S4,S4,S5,S5

- .............

###3、马尔可夫奖励过程(Markov Reward Process)

&ensp;&ensp;马尔可夫奖励过程可以看作为马尔可夫链与奖励的组合。在马尔可夫的奖励过程中，一般S是有限的状态集($s \in S$)，P是具体$P(S\_{t+1}=s^/|S\_t=s)$的转换模型，R是奖励期望函数$R(S\_t=s)=E[r\_t|S\_t=s]$表示到了某个状态s会有多大的奖励,$\gamma \in [0,1]$表示折扣因子。如果状态的数量有限，R可以被表示一个向量。

**奖励值返回的相关的概念：**

1. 范围定义：在每一条马尔可夫链中最大步数的数量，但是最大步数可能无法确定，如果可以确定则称为有限马尔可夫奖励过程。  	
2. 返回值的定义：从第t步到最大范围的折扣奖励和$$G\_t = R\_{t+1}+\gamma R\_{t+2}+\gamma^{2} R\_{t+3}+...+\gamma^{T-t-1} R\_{T}$$
3. 对于马尔可夫奖励过程中状态价值函数的定义：在状态s情况下，从t时刻可以返回的期望奖励总和$V\_t(s)=E[G\_t|S\_t=s]=E[R\_{t+1}+\gamma R\_{t+2}+\gamma^{2} R\_{t+3}+...+\gamma^{T-t-1} R\_{T}|S\_t=s]$，$V\_t(s)表示在当前状态未来奖励的值$

**为什么需要折扣因子$\gamma$**
1. 避免在马尔可夫环中会产生无穷的奖励
2. 未来的不确定性可能没有完全被考虑
3. 人类更看中的是当前的奖励，对将来的奖励不是很感兴趣
4. 如果$\gamma=0$则表示仅仅关心即时奖励，$\gamma=1表示将来的奖励重要性和当前一样$


**马尔可夫奖励过程举例：**

&ensp;&ensp;如Fig4，假设在s1状态奖励值为5，在s7状态的奖励值为10，其余状态的奖励值为0。则奖励R可以表示为$R=[5,0,0,0,0,0,10]$

&ensp;&ensp;假设我们限定马尔可夫链的长度为4，$\gamma = 1/2$，则返回的的奖励G根据路径不同奖励也不同。
$$return\quad s\_4,s\_5,s\_6,s\_7: 0+\frac{1}{2}\times0 + \frac{1}{4}\times0+\frac{1}{8}\times10$$
$$return\quad  s\_4,s\_3,s\_2,s\_1: 0+\frac{1}{2}\times0 + \frac{1}{4}\times0+\frac{1}{8}\times5$$
$$return\quad s\_4,s\_5,s\_6,s\_6:=0$$

&ensp;&ensp;我们可以找到很多链路来进行平均值的求解来计算S4的状态价值

###4、马尔可夫奖励过程中奖励的计算方法

**直接计算马尔可夫的奖励**

&ensp;&ensp;期望从状态s开始返回的奖励为$V(s)=E[G\_t|S\_t=s]=E[R\_{t+1}+\gamma R\_{t+2}+\gamma^{2} R\_{t+3}+...+\gamma^{T-t-1} R\_{T}|S\_t=s]$。MPR的价值函数满足下式中的贝尔曼等式$V(s) = \overbrace {R(s)}^\text{当下的奖励} + \overbrace {\gamma \sum\_{s^/ \in S}P(s^/|s)V(s^/)}^\text{将来的折扣奖励和}$，我们可以从下面的式子去推导贝尔曼方程$$V(s)=E[R\_{t+1}+\gamma E[R\_{t+2}+\gamma R\_{t+3}+...+\gamma^{T-t-2} R\_{T}]|S\_t=s]$$

Fig5

&ensp;&ensp;**贝尔曼等式的理解：**贝尔曼等式可以描述迭代的状态关系。$V(s) = R(s) + \gamma \sum\_{s^/ \in S}P(s^/|s)V(s^/)$，上图Fig5表示马尔可夫转移矩阵以及当前状态到下一个状态的转移概率。

Fig6

&ensp;&ensp;上图Fig6中可以表示V(S)的矩阵表示形式，$V=R+\gamma PV$，我们可以通过逆矩阵的计算得出$V=(I-\gamma P)^{-1}R$，但是这种计算仅适合在小的马尔可夫奖励过程，因为他的时间复杂度为$O(N^3)$

**用迭代的方法求解马尔可夫奖励过程的价值：**

1. 动态规划(Dynamic Programming)
2. 蒙特卡洛算法(Monte-Carlo evaluation)
3. T-D学习（1，2两种方法的组合） (Temporal-Difference learning)

Fig7

&ensp;&ensp;**蒙特卡洛算法：**我将他理解为均值求解算法，它具体的算法过程如Fig7所示，N表示选取链路的个数。假如我们计算$V(S\_4)$，
$$return\quad s\_4,s\_5,s\_6,s\_7: 0+\frac{1}{2}\times0 + \frac{1}{4}\times0+\frac{1}{8}\times10$$
$$return\quad  s\_4,s\_3,s\_2,s\_1: 0+\frac{1}{2}\times0 + \frac{1}{4}\times0+\frac{1}{8}\times5$$
$$return\quad s\_4,s\_5,s\_6,s\_6:=0$$
$$还可以得到更多的轨迹来计算均值$$

Fig8

&ensp;&ensp;**迭代算法：**如Fig8中，迭代算法其实用到的是贝尔曼的更新公式，也可以理解为套娃操作。

## 二、马尔可夫决策过程

###1、马尔可夫决策过程的相关定义

1. 马尔可夫决策过程可以理解为含有决定的马尔可夫奖励过程。
2. 相关参数的定义：
  1. S代表有限的状态集合
  2. A代表有限的行为集合(决策过程中多了一个行为定义)
  3. $P^a$是每个行为的转换模型$P(S\_{t+1}=s^/|S\_t=s,\color{red}{a\_t=a})$
  4. R代表奖励函数$R(s\_t=s,a\_t=a)=E[r\_t|s\_t=s,\color{red}{a\_t=a}]$
  5. 折扣因子$\gamma \in [0,1]$
3. 马尔可夫决策过程是一个元祖：$(S,A,P,R,\gamma)$

###2、策略（policy）在马尔可夫决策过程中的定义

1. 策略描述在每个状态下哪种行为应该被执行
2. 在给出的状态下，我们可以得出这状态对应不同行为的分布
3. policy:$\pi (a|S)=P(a\_t=a|s\_t=s)$
4. 策略是禁止的和时间无关，$A\_t~\pi (a|s)$对于任意的t>0都成立
5. 给出马尔可夫决策过程$(S,A,P,R,\gamma)$和一个策略$\pi$
6. 状态序列S1,S2....是一个马尔可夫过程$(S,P^{\pi})$(状态转移过程)
7. 状态和奖励序列S1,R1,S2,R2...是一个马尔可夫奖励过程$(S,P^{\pi},R^{\pi},\gamma)$(状态转移过程)，如下公式表示马尔可夫决策过程到奖励的转化$$P^{\pi}(s^/|s)=\sum\_{a \in A}\pi (a|s)P(S^/|s,a)$$
$$R^{\pi}(s)=\sum\_{a \in A}\pi (a|s)R(s,a)$$

###3、对比马尔可夫链/马尔可夫奖励过程和马尔可夫决策过程

Fig9

###4、马尔可夫决策过程的价值函数(状态价值，行为价值)
1. MDP的状态价值函数$v^{\pi}(s)$是从状态s开始，按照策略$\pi$获得的期望奖励。$$v^{\pi}(s)=E\_{\pi}[G\_t|s\_t=s]$$
2. MDP的行为价值函数$q^{\pi}(s,a)$是从状态s开始，采取行为a，按照策略$\pi$获得的期望奖励。$$q^{\pi}(s,a)=E\_{\pi}[G\_t|s\_t=s,a\_t=a]$$
3. 在$v^{\pi}(s)与q^{\pi}(s,a)$之间有相关联系
$$v^{\pi}(s)=\sum\_{a \in A}\pi (a|s)q^{\pi}(s,a)$$，可以看成先通过状态得到行为，再由行为和状态得出奖励，最后进行加和行为奖励函数。
4. 我们可以将状态价值函数和行为价值函数分解为当前状态的奖励和后续状态折扣价值之和。
$$v^{\pi}(s)=E\_{\pi}[R\_{t+1}+\gamma v^{\pi}(s\_{t+1})|s\_t=s]$$
$$q^{\pi}(s,a)=E\_{\pi}[R\_{t+1}+\gamma q^{\pi}(s\_{t+1},A\_{t+1})|s\_t=s,a\_t=a]$$
将上式表示成贝尔曼期望等式
$$v^{\pi}(s)=\sum\_{a \in A}\pi (a|s)q^{\pi}(s,a)$$
$$q^{\pi}(s,a)=R(s,a)+\gamma \sum\_{s^/ \in S}P(s^/|s,a)v^{\pi}(s^/)$$
因此将上式进行替代，我们可以得到当前状态和未来状态价值的关联，当前q和未来q之间的关联。
$$v^{\pi}(s)=\sum\_{a \in A}\pi (a|s)(R(s,a)+\gamma \sum\_{s^/ \in S}P(s^/|s,a)v^{\pi}(s^/))$$
$$q^{\pi}(s,a)=R(s,a)+\gamma \sum\_{s^/ \in S}P(s^/|s,a)\sum\_{a^/ \in A}\pi (a^/|s^/)q^{\pi}(s^/,a^/)$$

Fig10

Fig11

Fig10和Fig11用图解的方法解释了状态价值函数和行为价值函数

###5、策略评估
&ensp;&ensp;评估给定策略$\pi$的状态价值，通过计算$v^{\pi}(s)$；也被称为价值的预测。

 
###6、马尔可夫链和马尔可夫决策过程的区别举例
Fig12

&ensp;&ensp;Fig12中的小船若随波逐流则为马尔可夫链，若是有控制的滑行则为有agent的决策过程。


